{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7083b51d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Comprehensive Guide to Running PySpark Jobs on Google Cloud Dataproc\n",
    "\n",
    "This guide will walk you through everything you need to know to successfully run the provided `gcloud dataproc jobs submit pyspark` command. We'll cover prerequisites, a breakdown of each argument, and best practices.\n",
    "\n",
    "The command you provided is a powerful way to submit PySpark jobs to a Dataproc cluster. Let's break down each part and then set up the environment for a smooth execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a8187",
   "metadata": {},
   "source": [
    "### 0\\. Prerequisites\n",
    "\n",
    "Before you can run this command, ensure you have the following in place:\n",
    "\n",
    "  * **Google Cloud Project:** You need an active Google Cloud Project (e.g., `$PROJECT_ID `).\n",
    "  * **Billing Enabled:** Ensure billing is enabled for your Google Cloud Project.\n",
    "  * **gcloud CLI Installed and Authenticated:**\n",
    "      * If you don't have it, install the Google Cloud SDK: [https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install)\n",
    "      * Authenticate your `gcloud` CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcdcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud auth login\n",
    "REGION = europe-west1\n",
    "gcloud config set dataproc/region $REGION\n",
    "PROJECT_ID=$(gcloud config get-value project) && \\\n",
    "gcloud config set project $PROJECT_ID\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e213c749",
   "metadata": {},
   "source": [
    "  * **API Enabled:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f1a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud services enable dataproc.googleapis.com\n",
    "gcloud services enable servicemanagement.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdca2f8",
   "metadata": {},
   "source": [
    "\n",
    "  * **Cloud Storage Bucket:** A Google Cloud Storage (GCS) bucket (e.g., `gs://example-dataproc-workshop/01_2_Submit_Job/`) to store your PySpark scripts, utility files, and input/output data.\n",
    "  * **PySpark Scripts:**\n",
    "      * `process_data.py`: Your main PySpark application script located at `gs://example-dataproc-workshop/01_2_Submit_Job/pyspark_scripts/process_data.py`.\n",
    "      * `utils.py`: Any utility Python files your main script depends on, located at `gs://example-dataproc-workshop/01_2_Submit_Job/pyspark_scripts/utils.py`.\n",
    "  * **Dataproc Cluster:** A running Dataproc cluster (e.g., `example-cluster`) in the specified region (`europe-west1`). If you don't have one, see Section 2 for creating a cluster.\n",
    "  * **Data in GCS:** Input data for your PySpark job (e.g., sales data in `gs://example-dataproc-workshop/01_2_Submit_Job/raw_data/sales/`).\n",
    "  * **BigQuery Dataset:** If your job writes to BigQuery, ensure the BigQuery dataset (`example_dataproc_workshop`) exists.\n",
    "### 1. Creating Data for the Job\n",
    "\n",
    "We need a CSV file (since the script uses `.csv()`) with these columns. The `processing-date` argument was `2025-06-12`, so let's create a CSV file with some data for that date, and perhaps a few other dates to show filtering.\n",
    "\n",
    "Let's create a file named `sales_data.csv`.\n",
    "\n",
    "**`sales_data.csv` content:**\n",
    "\n",
    "```csv\n",
    "transaction_id,product_id,amount,transaction_date\n",
    "TID001,PROD_A,150.75,2025-06-12\n",
    "TID002,PROD_B,200.00,2025-06-12\n",
    "TID003,PROD_A,75.20,2025-06-11\n",
    "TID004,PROD_C,300.50,2025-06-12\n",
    "TID005,PROD_B,120.99,2025-06-13\n",
    "TID006,PROD_A,99.99,2025-06-12\n",
    "TID007,PROD_D,50.00,2025-06-11\n",
    "TID008,PROD_C,450.00,2025-06-13\n",
    "```\n",
    "\n",
    "\n",
    "1.  **Create the file locally:**\n",
    "    Open a text editor (like Notepad, VS Code, Sublime Text) and copy-paste the content above. Save the file as `sales_data.csv`.\n",
    "\n",
    "2.  **Upload the file to your GCS bucket:**\n",
    "    Use the `gsutil cp` command to upload this file to the specified input path:\n",
    "\n",
    "    ```bash\n",
    "    gsutil cp sales_data.csv gs://example-dataproc-workshop/01_2_Submit_Job/raw_data/sales/sales_data.csv\n",
    "    ```\n",
    "\n",
    "    **Note:** The `process_data.py` script is set to read from `gs://example-dataproc-workshop/01_2_Submit_Job/raw_data/sales/`. If you put `sales_data.csv` directly inside that directory, Spark will read all CSV files within it. You could also create a date-partitioned structure if you prefer, like `gs://example-dataproc-workshop/01_2_Submit_Job/raw_data/sales/2025-06-12/sales.csv`, and adjust your `input-path` argument accordingly if needed, but for this example, placing `sales_data.csv` directly in the `sales/` directory will work.\n",
    "\n",
    "Now, when your PySpark job runs with `--input-path=gs://example-dataproc-workshop/01_2_Submit_Job/raw_data/sales/` and `--processing-date=2025-06-12`, it will:\n",
    "\n",
    "1.  Read `sales_data.csv`.\n",
    "2.  Filter the data to include only rows where `transaction_date` is `2025-06-12`. This will result in:\n",
    "    ```\n",
    "    transaction_id,product_id,amount,transaction_date\n",
    "    TID001,PROD_A,150.75,2025-06-12\n",
    "    TID002,PROD_B,200.00,2025-06-12\n",
    "    TID004,PROD_C,300.50,2025-06-12\n",
    "    TID006,PROD_A,99.99,2025-06-12\n",
    "    ```\n",
    "3.  Group by `product_id` and sum the amounts. The expected output to BigQuery would be:\n",
    "\n",
    "| product\\_id | total\\_sales | processing\\_date |\n",
    "| :---------- | :----------- | :--------------- |\n",
    "| PROD\\_A     | 250.74       | 2025-06-12       |\n",
    "| PROD\\_B     | 200.00       | 2025-06-12       |\n",
    "| PROD\\_C     | 300.50       | 2025-06-12       |\n",
    "\n",
    "This data provides a good test case for the filtering and aggregation logic in your `process_data.py` script.\n",
    "\n",
    "### 2\\. Creating a Dataproc Cluster (if you don't have one)\n",
    "\n",
    "If you don't already have an `example-cluster` running, you can create one using the `gcloud` CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not done already\n",
    "REGION=europe-west1\n",
    "gcloud config set dataproc/region $REGION\n",
    "PROJECT_ID=$(gcloud config get-value project) && \\\n",
    "gcloud config set project $PROJECT_ID\n",
    "PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format='value(projectNumber)')\n",
    "\n",
    "\n",
    "\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "  --member=serviceAccount:$PROJECT_NUMBER-compute@developer.gserviceaccount.com \\\n",
    "  --role=roles/storage.admin\n",
    "\n",
    "gcloud compute networks subnets update default --region=$REGION --enable-private-ip-google-access\n",
    "\n",
    "\n",
    "# gcloud dataproc clusters create example-cluster \\\n",
    "#     --region=europe-west1 \\\n",
    "#     --zone=europe-west1-b \\\n",
    "#     --master-machine-type=n1-standard-4 \\\n",
    "#     --worker-machine-type=n1-standard-4 \\\n",
    "#     --num-workers=2 \\\n",
    "#     --image-version=2.1-debian11 \\\n",
    "#     --scopes=cloud-platform \\\n",
    "#     --project=$PROJECT_ID \n",
    "#     --max-age=1h\n",
    "\n",
    "\n",
    "gcloud dataproc clusters create example-cluster\\\n",
    "  --enable-component-gateway\\\n",
    "  --bucket=example-dataproc-workshop\\\n",
    "  --region=europe-west1\\\n",
    "  --no-address\\\n",
    "  --master-machine-type=n4-standard-2\\\n",
    "  --master-boot-disk-type=hyperdisk-balanced\\\n",
    "  --master-boot-disk-size=100\\\n",
    "  --num-workers=2\\\n",
    "  --worker-machine-type=n4-standard-2\\\n",
    "  --worker-boot-disk-size=200\\\n",
    "  --image-version=2.2-debian12\\\n",
    "  --optional-components JUPYTER\\\n",
    "  --max-age=3600s\\\n",
    "  --labels=mode=workshop,user=zelda\\\n",
    "  --project=$PROJECT_ID \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2d0558",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation of Cluster Creation Parameters:**\n",
    "\n",
    "  * `--region`: The Google Cloud region where your cluster will be created. Choose a region close to your data for better performance.\n",
    "  * `--zone`: (Optional but recommended) The specific zone within the region.\n",
    "  * `--master-machine-type`: Machine type for the master node. Choose based on your workload. `n1-standard-4` is a good starting point.\n",
    "  * `--worker-machine-type`: Machine type for worker nodes.\n",
    "  * `--num-workers`: Number of worker nodes. Adjust based on your processing needs.\n",
    "  * `--image-version`: Dataproc image version. Always good to use a recent stable version. `2.1-debian11` is a good choice for Spark 3.x.\n",
    "\n",
    "  * `--project`: Your Google Cloud Project ID.\n",
    "\n",
    "Cluster creation can take a few minutes. You can check the status with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud dataproc clusters describe example-cluster --region=europe-west1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df75274",
   "metadata": {},
   "source": [
    "\n",
    "### 3\\. Understanding the `gcloud dataproc jobs submit pyspark` Command\n",
    "\n",
    "Let's break down each argument of the job submission command:\n",
    "\n",
    "  * **`gcloud dataproc jobs submit pyspark gs://example-dataproc-workshop/01.2 Submit Job/pyspark_scripts/process_data.py`**\n",
    "\n",
    "      * `gcloud dataproc jobs submit pyspark`: This is the core command to submit a PySpark job to Dataproc.\n",
    "      * `gs://example-dataproc-workshop/01.2 Submit Job/pyspark_scripts/process_data.py`: This is the **path to your main PySpark application script** in Google Cloud Storage. Dataproc will download and execute this script on the cluster.\n",
    "\n",
    "  * **`--cluster=example-cluster`**\n",
    "\n",
    "      * Specifies the **name of the Dataproc cluster** where the job will be run. This cluster must already be running.\n",
    "\n",
    "  * **`--region=europe-west1`**\n",
    "\n",
    "      * Defines the **Google Cloud region** where your Dataproc cluster is located. This must match the region of your cluster.\n",
    "\n",
    "  * **`--project=$PROJECT_ID `**\n",
    "\n",
    "      * Indicates your **Google Cloud Project ID**. This ensures the command operates within the correct project context.\n",
    "\n",
    "  * **`--py-files=gs://example-dataproc-workshop/01.2 Submit Job/pyspark_scripts/utils.py`**\n",
    "\n",
    "      * This flag is used to include **additional Python `.py` files or `.zip` archives** that your main PySpark script depends on. These files will be distributed to the cluster and made available in the Python path for your PySpark application. You can specify multiple files separated by commas.\n",
    "\n",
    "  * **`--jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.23.2.jar`**\n",
    "\n",
    "      * This flag is crucial for including **external Java Archive (JAR) files** that your PySpark application needs. In this case, `spark-bigquery-with-dependencies_2.12-0.23.2.jar` is the connector for reading from and writing to Google BigQuery from Spark. This JAR is pre-packaged with necessary dependencies, making it easier to use.\n",
    "\n",
    "  * **`--properties=spark.executor.memory=4g,spark.sql.shuffle.partitions=50`**\n",
    "\n",
    "      * This allows you to set **custom Spark configuration properties**.\n",
    "          * `spark.executor.memory=4g`: Sets the amount of memory allocated to each Spark executor process. Adjust this based on your data size and processing needs. More memory can prevent OOM (Out of Memory) errors for memory-intensive tasks.\n",
    "          * `spark.sql.shuffle.partitions=50`: Determines the number of partitions to use when shuffling data for operations like `join` or `groupBy`. Adjusting this can significantly impact performance. Too few can lead to data skew and bottlenecks; too many can add overhead.\n",
    "\n",
    "  * **`--`**\n",
    "\n",
    "      * This double dash acts as a **delimiter**. Any arguments *after* `--` are treated as **arguments to your PySpark script** (`process_data.py`) itself, not as arguments to the `gcloud dataproc jobs submit` command.\n",
    "\n",
    "  * **`--input-path=gs://example-dataproc-workshop/01.2 Submit Job/raw_data/sales/`**\n",
    "\n",
    "      * An example argument **passed to your `process_data.py` script**. Your script will need to parse this argument to know where to read input data from.\n",
    "\n",
    "  * **`--output-table=example_dataproc_workshop.processed_sales`**\n",
    "\n",
    "      * Another example argument **passed to your `process_data.py` script**. This tells your script the BigQuery table where the processed data should be written.\n",
    "\n",
    "  * **`--processing-date=2025-06-12`**\n",
    "\n",
    "      * A third example argument **passed to your `process_data.py` script**. This could be used for date-based partitioning, filtering, or logging within your Spark job.\n",
    "\n",
    "### 4\\. Designing Your PySpark Script (`process_data.py`)\n",
    "\n",
    "Your `process_data.py` script needs to be designed to:\n",
    "\n",
    "1.  **Initialize a SparkSession:** This is the entry point for all Spark functionality.\n",
    "2.  **Parse Command-Line Arguments:** Use the `argparse` module (or similar) to parse the arguments passed after the `--` delimiter.\n",
    "3.  **Read Input Data:** Read data from the `--input-path` using Spark's DataFrame API.\n",
    "4.  **Perform Data Processing:** Apply your transformation logic.\n",
    "5.  **Write Output Data:** Write the processed data to the `--output-table` (likely BigQuery in this case, using the `spark-bigquery-connector`).\n",
    "6.  **Stop SparkSession:** Clean up resources.\n",
    "\n",
    "**Example `process_data.py` structure:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_data.py\n",
    "import argparse\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "# Import utilities if you have them in utils.py\n",
    "try:\n",
    "    from utils import some_utility_function\n",
    "except ImportError:\n",
    "    print(\"Could not import utils.py. Ensure it's in --py-files.\")\n",
    "    some_utility_function = lambda x: x # Fallback or handle appropriately\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Process sales data with PySpark.\")\n",
    "    parser.add_argument(\"--input-path\", type=str, required=True,\n",
    "                        help=\"GCS path to raw sales data (e.g., gs://example-dataproc-workshop/01_2_Submit_Job/raw_data/sales/).\")\n",
    "    parser.add_argument(\"--output-table\", type=str, required=True,\n",
    "                        help=\"BigQuery output table (e.g., example_dataproc_workshop.processed_sales).\")\n",
    "    parser.add_argument(\"--processing-date\", type=str, required=True,\n",
    "                        help=\"Date for processing data (YYYY-MM-DD).\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SalesDataProcessor\") \\\n",
    "        .config(\"temporaryGcsBucket\", \"example-dataproc-workshop\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "    # Define schema for input data (adjust as per your actual data)\n",
    "    input_schema = StructType([\n",
    "        StructField(\"transaction_id\", StringType(), True),\n",
    "        StructField(\"product_id\", StringType(), True),\n",
    "        StructField(\"amount\", DoubleType(), True),\n",
    "        StructField(\"transaction_date\", DateType(), True)\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        print(f\"Reading data from: {args.input_path}\")\n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .schema(input_schema) \\\n",
    "            .csv(args.input_path)\n",
    "\n",
    "        print(f\"Processing data for date: {args.processing_date}\")\n",
    "        # Example processing: Filter by date and calculate total sales\n",
    "        processed_df = df.filter(F.col(\"transaction_date\") == F.lit(args.processing_date)) \\\n",
    "                         .groupBy(\"product_id\") \\\n",
    "                         .agg(F.sum(\"amount\").alias(\"total_sales\")) \\\n",
    "                         .withColumn(\"processing_date\", F.lit(args.processing_date))\n",
    "\n",
    "        # Example of using a utility function\n",
    "        # processed_df = some_utility_function(processed_df)\n",
    "\n",
    "        print(f\"Writing processed data to BigQuery table: {args.output_table}\")\n",
    "        # Write to BigQuery\n",
    "        processed_df.write \\\n",
    "            .format(\"bigquery\") \\\n",
    "            .option(\"table\", args.output_table) \\\n",
    "            .mode(\"overwrite\").save()\n",
    "        # mode can be  \"append\" or \"overwrite\" or \"ignore\" or \"errorifexists\"\n",
    "\n",
    "        print(\"PySpark job completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        spark.stop()\n",
    "        raise # Re-raise the exception to indicate job failure\n",
    "\n",
    "    finally:\n",
    "        spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df153e3",
   "metadata": {},
   "source": [
    "**Example `utils.py` (if used):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe28f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def some_utility_function(df):\n",
    "    \"\"\"\n",
    "    Example utility function to add a timestamp column.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"processed_timestamp\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65336328",
   "metadata": {},
   "source": [
    "\n",
    "### 5\\. Uploading Your Files to GCS\n",
    "\n",
    "Make sure your `process_data.py` and `utils.py` (if applicable) are uploaded to your specified GCS path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil cp process_data.py gs://example-dataproc-workshop/01_2_Submit_Job/pyspark_scripts/process_data.py\n",
    "gsutil cp utils.py gs://example-dataproc-workshop/01_2_Submit_Job/pyspark_scripts/utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0a6bc",
   "metadata": {},
   "source": [
    "Also, ensure your input data exists at `gs://example-dataproc-workshop/01_2_Submit_Job/raw_data/sales/`. For example, you might have `sales_2025-06-12.csv` inside that directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac58b5c0",
   "metadata": {},
   "source": [
    "You want to create a BigQuery dataset named `processed_sales` within the `example-dataproc-workshop` project, and then a table within that dataset.\n",
    "\n",
    "Here's how you can do it using the `bq` command-line tool, which is part of the Google Cloud SDK and specifically designed for BigQuery interactions.\n",
    "\n",
    "### 1\\. Create the BigQuery Dataset\n",
    "\n",
    "First, let's create the dataset. Datasets need a location, and it's good practice to align it with your Dataproc cluster's region, which is `europe-west1`.\n",
    "\n",
    "```bash\n",
    "bq mk \\\n",
    "    --project_id=$PROJECT_ID  \\\n",
    "    --location=europe-west1 \\\n",
    "    example_dataproc_workshop.processed_sales\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "  * **`bq mk`**: The `bq` command-line tool's subcommand to \"make\" (create) a resource.\n",
    "  * **`--project_id=$PROJECT_ID `**: Specifies the Google Cloud project where the dataset will be created.\n",
    "  * **`--location=europe-west1`**: Sets the geographic location for the dataset. This is crucial as data cannot be moved between locations after creation, and your Spark job in `europe-west1` will perform best writing to a BigQuery dataset in the same region.\n",
    "  * **`example_dataproc_workshop.processed_sales`**: This is the full ID for your new dataset. It follows the format `[PROJECT_ID].[DATASET_ID]`. In this case, `example-dataproc-workshop` is assumed to be the project you are logically associating with this data, and `processed_sales` is the dataset ID.\n",
    "\n",
    "You should see output like:\n",
    "`Dataset '$PROJECT_ID :example_dataproc_workshop.processed_sales' successfully created.`\n",
    "\n",
    "### 2\\. Create the BigQuery Table\n",
    "\n",
    "Next, let's create the table `processed_sales` within the `example_dataproc_workshop.processed_sales` dataset. When creating a table, you need to define its schema.\n",
    "\n",
    "Based on the output of your `process_data.py` script:\n",
    "\n",
    "| Column Name     | Data Type |\n",
    "| :-------------- | :-------- |\n",
    "| `product_id`    | STRING    |\n",
    "| `total_sales`   | FLOAT     |\n",
    "| `processing_date` | DATE      |\n",
    "\n",
    "Here's the command to create the table with this schema:\n",
    "\n",
    "```bash\n",
    "bq mk \\\n",
    "    --table \\\n",
    "    --project_id=$PROJECT_ID  \\\n",
    "    --schema \"product_id:STRING,total_sales:FLOAT,processing_date:DATE\" \\\n",
    "    example_dataproc_workshop.processed_sales\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "  * **`bq mk --table`**: Specifies that you are creating a table.\n",
    "  * **`--project_id=$PROJECT_ID `**: Again, the project ID.\n",
    "  * **`--schema \"...\"`**: Defines the schema of your table.\n",
    "      * Each field is `name:TYPE`.\n",
    "      * Separate multiple fields with commas.\n",
    "      * Data types align with BigQuery's standard SQL types (e.g., `STRING`, `FLOAT`, `DATE`).\n",
    "  * **`example_dataproc_workshop.processed_sales`**: This is the full ID for your new table, in the format `[PROJECT_ID].[DATASET_ID].[TABLE_ID]`.\n",
    "\n",
    "You should see output like:\n",
    "`Table '$PROJECT_ID :example_dataproc_workshop.processed_sales' successfully created.`\n",
    "\n",
    "### Permissions\n",
    "\n",
    "Ensure the user or service account you are using with `gcloud` has the necessary BigQuery permissions (e.g., `BigQuery Data Editor`, `BigQuery User`, or `BigQuery Admin`) within the `$PROJECT_ID ` project to create datasets and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4fb815",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq mk \\\n",
    "    --project_id=$PROJECT_ID  \\\n",
    "    --location=europe-west1 \\\n",
    "    example_dataproc_workshop\n",
    "\n",
    "bq mk \\\n",
    "    --table \\\n",
    "    --project_id=$PROJECT_ID  \\\n",
    "    --schema \"product_id:STRING,total_sales:FLOAT,processing_date:DATE\" \\\n",
    "    example_dataproc_workshop.processed_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06c0a5",
   "metadata": {},
   "source": [
    "### 6\\. Running the Job\n",
    "\n",
    "Once all prerequisites are met and your files are in GCS, you can simply execute the `gcloud dataproc jobs submit pyspark` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed8d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud dataproc jobs submit pyspark \"gs://example-dataproc-workshop/01_2_Submit_Job/pyspark_scripts/process_data.py\" \\\n",
    "    --cluster=example-cluster \\\n",
    "    --region=europe-west1  \\\n",
    "    --project=$PROJECT_ID  \\\n",
    "    --py-files=\"gs://example-dataproc-workshop/01_2_Submit_Job/pyspark_scripts/utils.py\" \\\n",
    "    --properties=spark.bigquery.temporaryGcsBucket=example-dataproc-workshop \\\n",
    "    -- \\\n",
    "    --input-path=\"gs://example-dataproc-workshop/01_2_Submit_Job/raw_data/sales/\" \\\n",
    "    --output-table=example_dataproc_workshop.processed_sales \\\n",
    "    --processing-date=2025-06-12\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1b0a4",
   "metadata": {},
   "source": [
    "spark.bigquery.temporaryGcsBucket=example-dataproc-workshop to  --properties flag  tells the Spark BigQuery connector to use gs://example-dataproc-workshop/ as its temporary staging location for data being written to BigQuery. Make sure the service account running your Dataproc job has write permissions to this bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84468151",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 7\\. Monitoring Your Job\n",
    "\n",
    "After submitting the job, `gcloud` will provide a job ID (e.g., `job-20250702-123456-abcde`). You can monitor its progress:\n",
    "\n",
    "  * **gcloud CLI:**\n",
    "    ```bash\n",
    "    gcloud dataproc jobs describe job-20250702-123456-abcde --region=europe-west1\n",
    "    gcloud dataproc jobs wait job-20250702-123456-abcde --region=europe-west1 # To wait for job completion\n",
    "    ```\n",
    "  * **Google Cloud Console:** Navigate to **Dataproc \\> Jobs** in your project. You'll see a list of jobs, their status, and can click on a specific job to view its logs (driver output, YARN logs, etc.). This is often the easiest way to debug issues.\n",
    "\n",
    "### 8\\. Common Issues and Troubleshooting\n",
    "\n",
    "  * **Cluster Not Found/Running:** Ensure the cluster name and region are correct, and the cluster is in a `RUNNING` state.\n",
    "  * **Permissions Issues:**\n",
    "      * **GCS Access:** The Dataproc cluster service account (default: `project-number-compute@developer.gserviceaccount.com`) needs appropriate permissions to read/write from `gs://example-dataproc-workshop/01.2 Submit Job/`. Grant it `Storage Object Admin` or more granular roles like `Storage Object Viewer` for input and `Storage Object Creator` for output.\n",
    "      * **BigQuery Access:** If writing to BigQuery, the cluster service account needs `BigQuery Data Editor` or `BigQuery Data Owner` on the target dataset/table.\n",
    "  * **PySpark Script Errors:**\n",
    "      * Check the driver logs in the Cloud Console for stack traces and error messages.\n",
    "      * Ensure all necessary imports are present and paths are correct.\n",
    "      * Verify that the arguments parsed in your script (`input-path`, `output-table`, `processing-date`) match what's passed in the `gcloud` command.\n",
    "  * **`--py-files` not found:** Double-check the GCS path for `utils.py`.\n",
    "  * **`--jars` not found or incorrect:** Ensure the BigQuery connector JAR path is correct. If you're using a different Spark or Scala version, you might need a different JAR version.\n",
    "  * **Spark Properties:** Incorrect Spark properties can lead to performance issues or job failures.\n",
    "      * `spark.executor.memory`: If too low, you might see `OutOfMemoryError`.\n",
    "      * `spark.sql.shuffle.partitions`: Tuning this can improve performance.\n",
    "  * **Network Issues:** Ensure no firewall rules are blocking communication within the cluster or to external services.\n",
    "  * **Resource Exhaustion:** If your cluster is too small for the workload, jobs might fail or take a very long time. Consider scaling up your cluster (more workers, larger machine types).\n",
    "\n",
    "### 9\\. Best Practices\n",
    "\n",
    "  * **Version Control:** Keep your PySpark scripts and utility files under version control (e.g., Git).\n",
    "  * **Idempotency:** Design your PySpark jobs to be idempotent, meaning running them multiple times with the same inputs produces the same result. This is crucial for retries and recovery.\n",
    "  * **Structured Logging:** Implement good logging within your PySpark script to make debugging easier. Spark logs are already sent to Cloud Logging.\n",
    "  * **Testing:** Thoroughly test your PySpark scripts in a local Spark environment or smaller Dataproc cluster before running them on large production clusters.\n",
    "  * **Cost Management:**\n",
    "      * Choose appropriate machine types and number of workers for your workload.\n",
    "      * Consider using Preemptible VMs for worker nodes if your workload can tolerate interruptions (significant cost savings).\n",
    "      * Delete clusters when not in use, or use Dataproc's auto-deletion feature.\n",
    "  * **Cluster Reusability:** For frequent jobs, keeping a cluster running can save cluster startup time. For infrequent or long-running jobs, consider creating a new cluster for each job and deleting it afterward.\n",
    "  * **Security:** Grant the Dataproc service account only the necessary permissions (least privilege principle).\n",
    "  * **Monitoring and Alerting:** Set up Cloud Monitoring dashboards and alerts for Dataproc jobs to be notified of failures or performance issues.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
